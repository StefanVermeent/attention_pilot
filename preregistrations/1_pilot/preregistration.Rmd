---
bibliography: references.bib
csl: apa.csl
output:
  officedown::rdocx_document:
    page_margins:
      bottom: 1
      footer: 0
      gutter: 0
      header: 0.5
      left: 1
      right: 1
      top: 1
    plots:
      align: center
      caption:
        pre: 'Figure '
        sep: '. '
        style: Image Caption
    reference_docx: reference-doc.docx
    tables:
      caption:
        pre: 'Table '
        sep: '. '
        style: Table Caption
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}

# Load libraries
library(tidyverse)
library(officer)
library(flextable)
library(here)
library(pwr)



# set up chunk options
knitr::opts_chunk$set(
  echo = F,
  fig.align = "center",
  fig.pos = "!t", 
  out.extra = "",
  fig.show = "asis",
  message = FALSE,
  warning = FALSE
)

# set up flextable for tables
set_flextable_defaults(
  font.family = "Times", 
  font.size = 10,
  font.color = "black",
  line_spacing = 1,
  padding.bottom = 1, 
  padding.top = 1,
  padding.left = 1,
  padding.right = 1
)


# Source necessary objects
power_linear      <- read_csv(here("data", "1_pilot", "simulation", "power_linear_model.csv")) %>% mutate(power = round(power, 0))
power_mixed       <- read_csv(here("data", "1_pilot", "simulation", "power_mixed_model.csv")) %>% mutate(power = round(power, 0))
codebook          <- read_csv(here("data", "1_pilot", "pilot_self_report_codebook.csv"))
pilot_self_report <- read_csv(here("data", "1_pilot", "pilot_self_report.csv"))

```

# Introduction

Experiencing childhood adversity can have detrimental effects on cognitive outcomes [@hackman_2010; @ursache_2016]. However, the hidden talents approach suggests that people living in adversity might develop specific skills that help them navigate harsh and unpredictable environments [@ellis_2017; @frankenhuis_2013]. For example, recent work shows that growing up in stressful environments can enhance working memory updating, at least under certain conditions [@young_2018]. However, the cognitive mechanisms responsible for lowered (impairments) and improved (enhancements) performance on various tests remain poorly understood. Some studies suggest that specific types of adversity lead to specific abilities independent of the current context [@fields_2021; @nweze_2020]. Others only find enhancement effects under stressful [@mittal_2015; @young_2018] or more ecologically relevant [@young_submitted] testing conditions.

Further progress depends on addressing two related questions: First, to what extent are patterns of impairment and enhancement independent? Cognitive processes that are typically targeted in hidden talents studies (e.g., working memory, task-switching) are complex processes that share many of the same more basic processes [@conway_2021; @kovacs_2019; @verbruggen_2014]. This overlap means that impairment and enhancement patterns across tasks could be driven by the same underlying process. Second, which *cognitive* mechanisms are responsible for performance differences? Performance measures such as reaction times (RTs) or error rates are coarse indicators of performance. RTs on cognitive tasks are thought to reflect a number of sequentially unfolding processes. When the effect of adversity exposure on cognitive performance is measured by looking at RTs, enhancements or impairments might be driven by any of these subprocesses. Formal cognitive models such as the Drift Diffusion Model [DDM; @ratcliff_2008; @ratcliff_2015] might provide inside into the specific processes that are impaired or enhanced by adversity. 

Hidden talents research makes the often implicit assumption that enhanced abilities might reflect an externally focused orientation on the present [e.g., @ellis_2020; @frankenhuis_2016; @frankenhuis_2013]. For example, @frankenhuis_2016 argued that living in a harsh and unpredictable environment might require a stronger present-orientation (as opposed to being focused on the future) in order to deal with unpredictable threats and benefit from fleeting rewards. Empirical findings seem to point in the same direction. Enhanced performance on attention-shifting [@fields_2021; @mittal_2015] and working memory updating [@nweze_2020; @young_2018] have been explained in terms of an ability to track contextual changes, which is thought to help cope with changing information. One possibility that has yet to be empirically tested is that such effects are driven by a shared present and/or externally oriented attention mechanism. In this pilot study, we leverage the DDM to better understand the processes involved in performance differences, specifically by focusing on basic attention skills.

The DDM accounts for the fundamental cognitive processes underlying relatively simple, binary decision-making processes (2-AFC). The model assumes that the decision-making process constitutes a noisy information accumulation process that continues until one of two decision boundaries (corresponding to the response options) is reached. As soon as the accumulation process reaches a decision boundary, the process terminates and a motor response is initiated (e.g., pressing a key on the keyboard). The DDM accounts for the full pattern of correct RTs, incorrect RTs, and proportion of errors. In doing so, it recovers four parameters that map onto distinct cognitive processes [@voss_2004]. 

The accumulation process is modeled as a drift rate (*v*), reflecting basic cognitive speed/speed of information uptake. Large values of *v* reflect more efficient processing (faster and more accurate). The starting point (*z*) reflects the starting position of the accumulation process relative to the two decision boundaries. Unless people are biased to favor one response over the other (e.g., due to a skewed reward structure), (*z*) is usually fixed to be equidistant from both decision boundaries (*a*). Larger values of *a* indicate that the boundaries are further removed from each other, leading to more conservative responses (slower but more accurate) while smaller values of *a* reflect more liberal responses (faster but less accurate). Thus, *a* reflects the speed-accuracy trade-off made by an individual. The fourth and final parameter is the non-decision time (*Ter*), which includes processes that are not related to the decision process (e.g., initial stimulus encoding and initiating the motor response).

If people from adversity develop hidden talents, this should be reflected in specific DDM parameters. First and foremost, an enhanced cognitive ability would lead to more efficient information processing on tasks measuring that ability, which will be reflected in a higher drift rate *v* (or the other way around in case of a cognitive impairment). Second, enhanced performance could also be (partly) driven by the ability to quickly detect relevant stimuli before the onset of the decision-making process. Even if *v* is similar across levels of adversity, faster detection and encoding of relevant stimuli would allow individuals to start accumulating information sooner. This would be reflected in *Ter*. Finally, differences in performance across levels of adversity could be (partly) caused by the use of different strategies instead of by cognitive efficiency per se, for example, favoring speed over accuracy or vice versa. A tendency to adopt a specific strategy might be linked to individual differences in constructs like impulsivity. It is an open question whether performance differences driven by strategies should count as hidden talents [@young_2020].

The current study aims to address these questions using an incremental preregistration approach. In the current preregistered pilot study, we measured various forms of adversity with a focus on unpredictability on different time scales (e.g., seconds, days, years). In addition, participants completed three basic attention tasks (Flanker Task, Cued Attention Task, and Change Detection Task) that differed in the extent to which they would favor or punish a broad, externally focused attention style. We applied the DDM to the RT and accuracy data to decompose performance.

# Hypotheses

## Primary hypotheses

1)	People with more exposure to violence during childhood will show enhanced performance compared to people from safe environments on tasks that require quickly detecting peripheral stimuli and detecting subtle changes (i.e., the Cued Attention Task and Change Detection Task). This effect is explained by at least one of two DDM parameters: the speed of information uptake *v* and non-decision time *Ter*.
2)	People with more exposure to violence during childhood will perform worse on tasks that require ignoring distracting peripheral stimuli (i.e., the Flanker Task). This effect is at least partly explained by a wider initial attentional focus, (operationalized through the attentional width parameter of a special adaptation of the original DDM; see [data analyses](#ddm_plan) for details).

## Exploratory analyses

1) Develop an [analysis pipeline](#ddm_plan) to estimate DDM parameters for use as dependent variables.
2) Investigate the [factor structure](#efa) of several measures of unpredictability on different timescales through exploratory factor analysis (EFA). We then reran the analyses of the primary hypotheses using unpredictability as the independent variable, operationalized through composites of the original scales and through the factors resulting from the EFA.
3) Explore the role of state anxiety, hunger, and sleep deprivation as potential moderators of the relationship between adversity and attention performance. Anxiety might enhance attention performance by making participants more vigilant. Conversely, hunger and sleep deprivation could have a negative effect on performance.
4) Explore bivariate correlations between measures of adversity, attention, and measures of temporal orientation (i.e., impulsivity and future orientation).
5) Explore the correlation between current depressive symptoms and retrospective measures of adversity, which might reflect a negativity bias in recalling past events driven by current depressive symptoms [@nivison_2021].

# Methods {#methods}

## Participants {#participants}

Participants were 550 US-based individuals between the ages of 18 and 30 (*M* = `r #TBD`, *SD* = `r #TBD`). We decided for this age cut-off to reduce the effect of healthy age-related cognitive decline on speeded cognitive tasks, which becomes more pronounced after age 30 [@salthouse_2010]. Recruitment took place through Prolific Academic [www.prolific.co]. The sample was balanced to include roughly 50% males. Participants were eligible for the study if they were from the United States and if they spoke fluent English.

We conducted a power simulation using the *faux* package in R [@debruine_2021] to determine the optimal number of participants to include (more information including all simulation code on [Github](https://github.com/StefanVermeent/attention_project/tree/preregistration1/preregistrations/1_pilot/scripts)). We ran simulations for two scenarios: A linear regression model to test the effect of adversity (standardized) on attention performance (standardized) and a mixed regression model to test the adversity (standardized) X task condition (sum-coded) interaction, including a random intercept for subjects. We ran simulations for a standardized $\beta$ of 0.10 and 0.15. In addition, we simulated the power to detect these effects with imperfectly recovered DDM parameters, assuming a recovery accuracy (i.e., correlation between data-generating parameter value and actually recovered parameter value) of *r* = 0.85 (see the [Github link](https://github.com/StefanVermeent/attention_project/tree/preregistration1/preregistrations/1_pilot/scripts) for more information).

In determining the final sample size, we weighed the simulation results together our uncertainty about certain model parameters (mostly the fixed effects and sigma term for the mixed model) and the fact that this was a first, mostly exploratory pilot study. For the mixed model, we found that power was > .80 for the adversity X task condition interaction with *N* = 450 or more. For a linear main effect, detecting an effect of $\beta$ = 0.15 with .90 power would require *N* = `r pwr.r.test(r = 0.15, sig.level = 0.05, power = .90, alternative = "two.sided")[['n']] %>% round(0)`. When DDM parameter recovery accuracy was *r* = .85, power was `r (power_linear %>% filter(n_subjects == 500, fixed_effect == 0.15, effect == "linear_recov_main_power") %>% pull(power))/100` for *N* = 500 and $\beta$ = 0.15. Power was `r (power_linear %>% filter(n_subjects == 500, fixed_effect == 0.1, effect == "linear_recov_main_power") %>% pull(power))/100` under the same scenario when $\beta$ was 0.10. Ultimately, we decided on sampling 550 participants with an anticipated final sample size of around 500.

## Exclusion criteria {#exclusion}

We applied several exclusion criteria prior to analyzing the data. First, we excluded participants who did not complete the full study and those who had incomplete data on any of the attention tasks (*N* = `r #TBD `). Second, we analyzed responses to the attention checks and reversed coded items in the questionnaire part of the experiment. We excluded participants if they missed both attention check items or if they had suspicious response patterns (e.g., consistently endorsing high response options even when some items were reverse coded) (*N* = `r #TBD `). 

In addition, we screened the reaction times on each of the three attention tasks. For the Drift Diffusion analyses, it is important that each response is generated by a process of active information accumulation (i.e., a diffusion process, as opposed to guessing). To this end, trials with reaction times < 250 ms or > 3500 ms [@ratcliff_2015] were excluded from the analyses (*N* = `r #TBD `). Participants with more than 10 removed trials were excluded from the analyses (N = `r #TBD `).  

The exclusion criteria listed above are our best *a-priori* guesses. However, we expect to develop additional criteria empirically after collecting the data. 




## Measures {#measures}

The attention tasks were programmed in JsPsych version 3.6.1 [@deLeeuw_2015]. 

### Cued Attention Task {#cued_attention}

The Cued Attention Task was an adaptation of the Posner task, which measures basic spatial attention speed [@posner_1980]. On each trial, a left- or right-pointing arrow was presented in one of eight random locations at a distance of 300 pixels from the center of the screen (see Figure \@ref(fig:figure1)a). Participants had to indicate the direction of the arrow by pressing either the left- or right arrow key on their keyboard. On 50% of the trials, the arrow was preceded by a cue ('\*') in the exact same location (i.e., 'cued' trials). On the other 50% of the trials, the cue ('\*') was presented at the center of the screen (i.e., 'neutral' trials). Thus, on the cued trials, the location of the cue deterministically predicted the location of the arrow, whereas on the neutral trials the arrow always appeared in a different location.

Each trial started with a fixation cross presented at the center of the screen for 1000 ms. Then, the cue was presented for 250 ms either at the center of the screen or in the location where the arrow would appear next. Finally, the arrow was presented 250 ms after the onset of the cue and shown until a response was given. The Cued Attention Task began with eight practice trials (four cued and four neutral trials) in which participants received performance feedback after each trial (either "Correct!" printed in green or "Incorrect!" printed in red). After finishing the practice round, participants completed a single test block consisting of a total of 64 trials, consisting of two repetitions of all parameter combinations: arrow location (top center vs. top left vs. center left vs. bottom left vs. bottom center vs. bottom right vs. center right vs. top right) X arrow direction (left vs. right) X condition (cued vs. neutral). The main performance measures are RTs and accuracy for each condition.


### Flanker Task {#flanker}

The Flanker Task measures selective attention and response inhibition [@eriksen_1974]. On each trial of the Flanker task, participants were presented with a set of five arrows pointing either left or right. Their job was to indicate the direction of the central arrow while ignoring the flanking arrows to the left and right (see Figure \@ref(fig:figure1)b). On 50% of the trials, the flanking arrows pointed in the same direction as the central arrow (i.e., congruent trials), and on the other 50% of the trials they pointed in the opposite direction (i.e., incongruent trials). The arrows were randomly presented in the top-half or bottom-half of the screen. This was done to prevent participants from rigidly fixating on the center of the screen, which might reduce the interfering effect of the flanking arrows.

Each trial started with a fixation cross presented at the center of the screen. After a delay of 1000ms, the arrows are presented either in the top- or bottom-half of the screen. The Flanker task began with eight practice trials in which participants received performance feedback after each trial (either "Correct!" printed in green or "Incorrect!" printed in red). After finishing the practice round, participants completed a single test block consisting of a total of 64 trials, consisting of eight repetitions of all parameter combinations: Arrow location (top vs. bottom) X central arrow direction (left vs. right) X condition: (congruent vs. incongruent). Participants did not receive performance feedback during the test block. The main performance measures were RTs and accuracy for each condition.

### Change Detection Task {#change}

The Change Detection Task measures the ability to detect subtle spatial changes. On each trial of the change detection task, participants were presented with five colored circles against a gray background, each with a radius of 15 pixels (see Figure \@ref(fig:figure1)c). Each circle was located in a semi-random location around the central fixation cross. The location of each circle was sampled within a pre-specified area of 50 by 50 pixels to prevent overlap. Participants had 1000ms to memorize the locations of the five circles. After 1000ms, the circles disappeared for 500ms and then reappeared again. On 50% of the trials, one of the circles had moved to another location with a fixed displacement of 40 pixels in a 360 degree direction. On the other 50% of the trials, all circles were still in the same location as the initial memory display. Participants had to indicate whether they thought one of the circles changed location (by pressing the left-arrow key) or whether they thought all circles were still in the same location (by pressing the right-arrow key). Note that the only potential difference between the memory and probe display on each trial was the displacement of **one** circle; the remaining circles were always in the same place and circles never changed color within a trial.

The change detection task started with five practice trials in which participants received performance feedback after each trial (either "Correct!" printed in green or "Incorrect!" printed in red). After finishing the practice round, participants completed two test blocks consisting of 25 trials each (50 trials in total). The design was fully counterbalanced so that each circle moved on five trials and was a different color each time it moved *between* trials. Participants did not receive performance feedback during the two test blocks. The main performance measures are RTs and accuracy across all trials.


```{r figure1, fig.width=6, dpi=600, fig.id = "figure1", fig.cap.style = "Image Caption", fig.cap="Overview of the three cognitive tasks."}
knitr::include_graphics("figure1.png", error = F)
```


### Current state {#state}

We assessed state anxiety during the experiment using the state subscale of the State-Trait Anxiety Inventory [STAI-S; @spielberger_1999]. The STAI-S contains 20 short items measuring current anxiety (e.g., "I feel tense"; See [Table S1](#appendix) for an overview of all items). Participants rated each item on a scale of 1 (not at all) to 4 (very much so). An overall state anxiety variable was computed by averaging across the 20 unweighted items (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `).

In addition, participants answered five questions relating to specific states: "Are you currently sick?" (rated as yes or no); "Have you eaten a full meal today?" (rated as yes or no); "How hungry do you feel right now?" (rated from 1 (not at all) to 5 (very hungry)); "How well did you sleep last night?" (rated from 1 (very poorly) to 5 (very well)); "How rested or refreshed did you feel when you woke up this morning?" (rated from 1 (not at all) to 5 (very rested)). We computed an overall sleep deprivation composite by standardizing and averaging across the two unweighted sleep-related items (*M* = `r #TBD `, *SD* = `r #TBD `).

### Violence exposure {#violence}

Violence exposure was measured using the Neighborhood Violence Scale [NVS; @frankenhuis_2018; @frankenhuis_deVries_2020] and two items assessing exposure to physical fights before age 13. The NVS contains seven items measuring perceived exposure to violence before age 13 (e.g., "Crime was common in the neighborhood where I grew up"; See [Table S2](#appendix) for an overview of all items). Participants rated each on a scale from 1 (never true) to 5 (very often true). The physical fighting items assessed the number of times participants witnessed fights before age 13: "Based on your experiences, how many times did you see or hear someone being beaten up in real life, before age 13?" and "How many times were you in a physical fight, before age 13?" Answers to both items ranged from 1 (0 times) to 8 (12 or more times). The items of the NVS were averaged together (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `). Similarly, we averaged the scores on the two fighting items together. Finally, we created a perceived violence exposure composite by standardizing the NVS and fighting composites and calculating an unweighted average (*r* = `r #TBD `).

### Unpredictability {#unpred}

Perceived unpredictability before age 13 was measured using two different scales: A scale of perceived childhood unpredictability used in previous research [@mittal_2015; @young_2018], an adaptation of the Questionnaire of Unpredictability in Childhood [QUIC; @glynn_2019], and two scales measuring change in the family and social environment. The perceived childhood unpredictability scale contained eight items measuring perceived unpredictability before age 13 (e.g, "My family life was generally inconsistent and unpredictable from day-to-day"; See [Table S3](#appendix) for an overview of all items). Participants rated each on a scale from 1 (never true) to 5 (very often true).

We included the QUIC because it captures several dimensions of environmental and household unpredictability. The QUIC distinguishes between items capturing more short-term unpredictability (i.e., on the level of seconds to minutes) and more long-term unpredictability (i.e., on the level of days to months). The scale of perceived childhood unpredictability combines items on different time scales. Some items measure unpredictability on a relatively short-term scale (e.g., item 1) while other items measure unpredictability on a long-term scale (e.g., item 3). Unpredictability on different time scales might have different effects on the development of specific attention styles. 

We included all five scales of the QUIC: 1) Parental monitoring and involvement, 2) Parental predictability, 3) Parental environment, 4) Physical environment, and 3) Safety and security (See [Table S4](#appendix) for an overview of all items). The items of the Parental environment subscale deviated most from the original. 


We made three general changes to the original scale as described in @glynn_2019. First, we adapted all items to refer to experiences before age 13. This was done to reduce cognitive load from having to go back-and-forth between different time scales. Second, most items were rated on a scale of 1 (never true) to 5 (very often true) instead of the original yes/no answer format. An exception was made for four items of the parental environment scale which asked for more specific experiences (e.g., "I experienced changes in my custody arrangement"; see [Table S4](#appendix)). For these items, we adopted a response scale with the options "never", "only once", "a couple times", "several times", "many times". Third, quantifiers such as "frequently", "often", and "There was a period of time when [...]" were dropped to better match the response scale. We excluded the item "My parents got divorced" because it did not fit the response format and was captured by one of the items of the perceived unpredictability scale.

We computed several unweighted unpredictability variables to be able to assess how they would relate to the dependent variables as well as to each other: 1) overall perceived unpredictability variable containing all eight items of the perceived unpredictability scale (*M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `); 2) an overall QUIC variable by averaging all 37 items (*M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `); 3) five separate QUIC subscales by averaging their sub-items (Parental monitoring and involvement: *M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `; Parental predictability: *M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `; Parental environment: *M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `; Physical environment: *M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `; Safety and security: *M* = `r #TBD `, *SD* = = `r #TBD `, $\alpha$ = `r #TBD `). See also [below](#efa) for details on a factor analysis based on the individual items across scales.

One additional scales measured the stability of the family and social environment. On a scale of 1 (the same all the time) to 5 (constant and rapid changes), participants indicated how often the following aspects of their family and social environment changed before age 13: 1) economic status, 2) family environment, 3) childhood neighborhood environment, and 4) childhood school environment. 

Finally, we included four objective measures of unpredictability before age 13. Participants provided answers to the following items in an open response format: 1) "How often did you move?"; 2) "How many adults lived in your home on average?"; 3) "How many romantic partners did your mother have (not counting your father)?"; 4) "How many romantic partners did your father have (not counting your mother)?". 

### Household chaos {#chaos}

We assessed household chaos before age 13 using an adaptation of the Confusion, Hubbub, and Order Scale [CHAOS; @matheny_1995]. The CHAOS consists of 15 items measuring the level of chaos in the household (e.g., "No matter how hard we tried, we always seemed to be running late"; See [Table S5](#appendix) for an overview of all items). We made two changes to the original scale as described in @matheny_1995. First, all items were converted from the present tense to the past tense, and were endorsed as applying to participants' lives before age 13. Second, all items were rated on a scale of 1 (never true) to 5 (very often true) instead of the original yes/no answer format. This change was also implemented to reduce cognitive load by keeping the answer options the same between scales. An overall household chaos variable was computed by averaging the 15 unweighted items (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `).

### Poverty exposure {#ses}

Participants' perceived level of resource scarcity before age 13 was measured using seven items (e.g., "Your family had enough money to afford the kind of home you all needed"; See [Table S6](#appendix) for an overview of all items). Participants rated each item on a scale from 1 (never true) to 5 (very often true). Scores for the first six items were reverse coded so that higher scores indicated more perceived resource scarcity. The items were averaged together to create an unweighted composite scale (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `).

In addition, we measured several indicators of objective SES before age 13. First, participants separately indicated the highest education of their mother and father on an 8-point scale: 'some high school', 'GED', 'high school diploma', 'some college but no college degree', associate's  degree', 'bachelor's degree', 'master's  degree', or 'doctoral or lab degree'. The mother and father education level were averaged to create an overall unweighted parental education composite (*M* = `r #TBD `, *SD* = `r #TBD `). Participants also indicated their family's household income before age 13 on a 6-point scale: 'less than \$ 25k/year', '\$25k - \$49k/year, '\$50 - \$74k/year', '\$75 - \$99k/year', '\$100 - \$149k/year', 'more than \$150k/year'. Scores were reverse coded so that higher scores indicated higher levels of poverty.

We created a composite score of poverty exposure before age 13 by averaging together the standardized scores of perceived level of resource scarcity, overall parental education, and household income.



### Impulsivity {#impulsivity}

We assessed impulsivity with the Motor Impulsivity subscale of the Barrett Impulsivity Scale (BIS; short form; [@patton_1995; @spinella_2007]). The Motor Impulsivity subscale of the BIS consists of five items (e.g., "I do things without thinking"; See [Table S7](#appendix) for an overview of all items). We did not include the Non-planning subscale because it overlapped substantially with the Future Orientation Scale described below. In addition, we did not include the Attention impulsivity subscale because it included items which we deemed to be mostly irrelevant for our target population (e.g., "I 'squirm' at plays or lectures"). We changed the original 4-point rating scale (rarely/never to almost always) to a 5-point rating scale ranging from 1 (never true) to 5 (very often true). An overall impulsivity variable was computed by averaging the five unweighted items (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `). 

### Future Orientation {#future_orientation}

We assessed future orientation with an adapted version of the Future Orientation Scale [FOS; @steinberg_2009]. The original scale consists of 15 sets of opposing items separated by "BUT" (e.g., "Some people like to plan things out one step at a time BUT other people like to jump right into things without planning them out beforehand"). Participants first choose the item that best matches their general preference, and then indicate whether the statement is "really true" or "somewhat true". We adapted this format in a couple of ways. First, we converted the two statements per item to a single statement by picking the statements in the original right-hand column. Second, we adapted the 15 statements from a third-person to a first-person format. These changes were made in an attempt to reduce the cognitive load of the items. We worried that people with less formal education or who were sitting in a noisier environment would struggle with the length of the original items. 

In addition, item 8 of the original scale ("[...] other people would rather spend their money right away on something fun than save it for a rainy day") was changed to "I'd rather spend money right away than save it for a rainy day" (i.e., dropping the phrase "on something fun") to make it more general with regard to the thing that money is spent on. For people from adversity, spending money right now instead of saving it for the future might often be born out of necessity (e.g., having just enough money for food and shelter; being in debt) instead of a failure to delay gratification. See [Table S8](#appendix) for an overview of all adapted items). Finally, the rating scale was adapted from the original 4-point scale (ranging from really true for the left-hand statement to really true for the right-hand statement) to a 5-point scale ranging from 1 (never true) to 5 (very often true).

An overall future orientation variable was computed by averaging the 15 unweighted items (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `). In addition, we calculated subscales for "Planning ahead" (items 1, 6, 7, 12 and 13), "Time perspective" (items 2, 5, 8, 11 and 14) and "Anticipation of future consequences" (items 3, 4, 9, 10 and 15).

### Depressive symptoms {#depression}

We assessed depressive symptoms during the past week using the Center for Epidemiologic Studies Depression Scale [CESD; @radloff_1977]. The scale consists of 20 items (e.g., "I do things without thinking"; See [Table S9](#appendix) for an overview of all items). Participants rate each item on a scale of 1 (rarely or none of the time (less than 1 day)) to 4 (most or all of the time (5-7 days)). An overall depression variable was computed by averaging the 20 unweighted items (*M* = `r #TBD `, *SD* = `r #TBD `, $\alpha$ = `r #TBD `).

## Procedure {#procedure}

The experiment was completed on the participants' own laptop or desktop computer and consisted of five parts: consent, attention task battery, questionnaire battery, brief demographics form, and final checks including the opportunity to give feedback on the experiment. Participants were allowed to refrain from answering any of the questionnaire items, but were prompted with a warning once when moving to the next page if one of the items was not answered (which they could ignore).

After providing consent, participants started with the three attention tasks. They were asked to complete the attention tasks in a quiet room in the house where they would be least likely to be distracted by other people or outside noises. The order of the tasks was counterbalanced between subjects. At the onset of the first task, the experiment went into full-screen mode to limit distractions from other programs or browser tabs. The size of the task stimuli was controlled between subjects using the resize plugin in JsPsych [@deLeeuw_2015]. Participants were asked to hold a creditcard (or similarly sized card) up against the screen and to increase the size of a blue rectangle on the screen until it matched the size of the creditcard. The stimulus display for each task was resized so that 100 pixels corresponded to 1 inch for all participants. After successfully resizing the screen, participants completed all three tasks. During the task, the cursor was hidden from the screen to minimize distractions. Code for the three tasks can be found on [Github](https://github.com/StefanVermeent/attention_project/tree/main/tasks). 

After completing the attention tasks, participants completed the questionnaire battery in the following fixed order: 1) Current state (state anxiety and separate questions relating to specific states); 2) Childhood adversity (perceived unpredictability, perceived socio-economic status, exposure to violence and physical fights, household chaos); 3) Temporal orientation (impulsivity, future orientation); 4) Depressive symptoms.

Finally, the demographics questions asked about the participant's age, weight, height, physical activity, sex at birth, gender, ethnicity, social class (current and during childhood), education level (one's own as well as parents' education level), occupation, and household income (current and during childhood). At the end of the experiment, we asked participants if they ever got up or were interrupted during the study, and how noisy their environment was during the attention tasks. 

The full experiment was expected to take approximately 35 minutes. Participants were paid Â£4.38 upon completing the full experiment.

## Primary data analyses {#analysis_plan}

### Reaction Times {#rt_plan}

For the Cued Attention Task and Flanker Task, average reaction times per participant were calculated separately for each condition (cued/neutral trials and incongruent/congruent trials, respectively). For the change detection task, we calculated an average reaction time across all trials. If necessary for meeting model assumptions of normally distributed residuals, reaction times were log-transformed. We only planned on performing analyses on the proportion of errors if the average error rate was at least 5% of trials. Note that the DDM analysis described below provides a full breakdown of performance both in terms of reaction times and accuracy. For the Cued Attention Task and Flanker Task, we performed a set of linear mixed effects analyses to test adversity X condition interactions on mean RTs and accuracy (if possible). Condition was sum-coded (-1 and 1). All models included a random intercept for participants. For the Change Detection Task, we performed linear regression to test the main effect of adversity on mean RTs and accuracy (if possible). Across separate models, adversity was operationalized as the composite measures of violence exposure, SES, perceived childhood unpredictability, household chaos, and the QUIC subscales (unless they correlated > .80, in which case QUIC subscales were merged together). Significant interaction effects were unpacked using simple slopes analysis.

### Drift Diffusion Model {#ddm_plan}

DDM models were fit separately to the data of each participant. 
(see also our power simulations on [Github](https://github.com/StefanVermeent/attention_project/tree/main/preregistrations/1_pilot/scripts), which indicated that recovered drift rate and boundary separation parameters generally correlated highly (*r* = .80-.95) with the true data-generating parameters even with 30 trials).

The DDM was fit separately to the data of each participant. For the data of the Change Detection Task and the Cued Attention Task, the DDM was fit using Fast-dm [@voss_2007]. We used Maximum Likelihood estimation as it has been shown to provide reliable estimates with relatively few trials [@lerche_2017] (see also our power simulations on [Github](https://github.com/StefanVermeent/attention_project/tree/main/preregistrations/1_pilot/scripts), which indicated that recovered drift rate and boundary separation parameters generally correlated highly (*r* = .80-.95) with the true data-generating parameters even with 30 trials). Parameters *a*, *v* and *Ter* were freely estimated and *z* was fixed to 0.5 (the midpoint, indicating no response bias). In addition, all inter-trial variability parameters were fixed to 0 to increase model parsimony. For the Cued Attention Task, the DDM was fit separately to data of the two conditions (cued vs neutral). For the Change Detection task, we had no *a-priori* expectations about performance differences between the change and no-change trials. Prior to fitting the DDM models, we ran paired-sample *t*-tests on the RTs and proportion of errors. If either model showed significant mean differences, the DDM was fit separately to data of the change and no-change trials. If there were no differences, the DDM models were fit to all trials simultaneously.

The DDM analysis of the Flanker task was done using an adaptation of the standard DDM that was specifically developed to account for Flanker data: The Spotlight Drift Diffusion Model [SDDM; @grange_2016; @white_2018a; @white_2018b; @white_2011]. The standard DDM model assumes a stable drift rate over time within trials (i.e., the rate of information accumulation does not change over time). However, on the Flanker Task, reaction time patterns indicate that the drift rate increases over time, thus violating the basic assumption of the standard DDM [@white_2011]. The SDDM explains this pattern by assuming that attention on the Flanker task works like a spotlight that starts relatively broad, and then gradually narrows down to the central arrow over time. Each arrow provides perceptual input *p*. If the flanking arrows are congruent, all flankers take a positive value for *p*; if the flanking arrows are incongruent, they take a negative value for *p* while the central arrow takes a positive value. The drift rate (*v*) at each time point is the function of the strength of perceptual input *p* multiplied by the amount of attention focused on each arrow. 

Attention is operationalized by two additional parameters: the initial width of the attentional spotlight (*SD~a~*) and the shrinking rate of the attentional spotlight (*rd*). The attentional spotlight is assumed to be normally distributed over the arrows, with relatively more attention paid to the arrows at the center and relatively less attention paid to the peripheral arrows. Over time, this normal distribution (*SD~a~*) narrows down to the central arrow at the rate specified by *rd*, thereby gradually decreasing the interfering effect of the flankers.

Previous research has shown that it is not useful to interpret both *SD~a~* and *rd* because they trade off with each other [i.e., a wide initial spotlight combined with a large shrinking rate leads to similar patterns as a narrow initial spotlight combined with a low shrinking rate; @white_2018a; @white_2011]. Most previous studies have fixed *SD~a~* and only estimated *rd*, or calculated a *SD~a~*/*rd* ratio, which serves as a measure of interference. However, our primary interest lies in the initial width of the attentional spotlight, which we hypothesize to be wider for people from harsh and unpredictable environments compared to people from safe and predictable environments. In this pilot study, we fitted several SDDM models either estimating both attention parameters or fixing *rd* to see how it affected model fit (more details below). Model fit was done using the *flankr* package in R [@grange_2016].

DDM parameters *v*, *a* and *Ter* (as well as *p*, *SD~a~* and *SD~a~*/*rd* for the Flanker Task) were used as dependent variables in the analyses for each attention task. For the Flanker Task and Change Detection Task, DDM parameters were submitted to a series of linear regression models with adversity as the independent variable. For the Cued Attention Task, DDM parameters were submitted to linear mixed effects models to test for adversity X condition interactions. All models included a random intercept for participants. Independent adversity variables were identical to the RT/accuracy analyses. Significant interaction effects were unpacked using simple slopes analysis.

### Factor structure of unpredictability {#efa}

The included unpredictability measures contain items that vary in timescale, ranging from features of the childhood environment that might have been unpredictable on a daily basis (e.g., "things were often chaotic in my house") to sources of unpredictability that typically unfold on a timescale of weeks or months (e.g., "At least one of my parents changed jobs frequently"). Since attention is involved in information processing on a timescale of seconds to minutes, we expect that attention styles should be more strongly related to sources of unpredictability that unfold on a similar timescale. However, it is unclear whether current retrospective measures of unpredictability are able to differentiate between different timescales. In addition, it might be the case that items of unpredictability on a longer timescale are good proxies for unpredictability on a a more daily timescale. We conducted an exploratory factor analysis (EFA) to address these issues.

EFA was used to find common latent factors underlying the unpredictability/household chaos measures: perceived unpredictability, QUIC, and CHAOS, stability of the family and social environment, and the objective measures of unpredictability. The number of factors to retain was based on parallel analysis, with the added requirement that each factor was composed of at least five items that had their strongest loading on that particular factor. Factor loadings above 0.32 were considered meaningful (Tabachnick & Fidell, 2014). We anticipated the factors to be correlated substantially, and therefore planned to use oblimin rotation.

\pagebreak

# References {#refs}

<div id="refs"></div>

\pagebreak

# Appendix

Go back to [Methods](#state)

```{r supp_table1, tab.id="tableS2", tab.cap.style="Table Caption", tab.cap="Items of the State-Trait Anxiety Inventory (state subscale; STAI-S)"}
codebook %>%
  filter(str_detect(Variable, "stai_s(\\d\\d|_\\d\\d)")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  mutate(Label = ifelse(Item %in% c(1,2,5,8,10,11,15,16,19,20), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```

\pagebreak

Go back to [Methods](#violence)

```{r tableS2, tab.id="tableS2", tab.cap.style="Table Caption", tab.cap="Items of the Neighborhood Violence Scale (NVS)"}
codebook %>%
  filter(str_detect(Variable, "violence\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  mutate(Label = ifelse(Item %in% c(1,3), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```

\pagebreak

Go back to [Methods](#unpred)

```{r tableS3, tab.id="tableS3", tab.cap.style="Table Caption", tab.cap="Items of the Perceived Childhood Unpredictability scale"}
codebook %>%
  filter(str_detect(Variable, "unp\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description")
```

\pagebreak

Go back to [Methods](#unpred)

```{r tableS4, tab.id="tableS4", tab.cap.style="Table Caption", tab.cap="Items of the Questionnaire of Unpredictability in Childhood (QUIC)"}
codebook %>%
  filter(str_detect(Variable, "quic\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  add_row(.before = 1, "Label" = "Parental monitoring and involvement") %>%
  add_row(.after = 10, "Label" = "Parental predictability") %>%
  add_row(.after = 23, "Label" = "Parental environment") %>%
  add_row(.after = 30, "Label" = "Physical environment") %>%
  add_row(.after = 38, "Label" = "Safety and security") %>%
  mutate(Label = ifelse(Item %in% c(1,2,3,4,5,6,7,8,9,11,14,16,22,32), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  merge_h_range(i = c(1,11,24)) %>%
  bold(i = c(1,11,24, 31, 39)) %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```

\pagebreak

Go back to [Methods](#chaos)

```{r tableS5, tab.id="tableS5", tab.cap.style="Table Caption", tab.cap="Items of the Confusion, Hubbub, and Order Scale (CHAOS)"}
codebook %>%
  filter(str_detect(Variable, "chaos\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  mutate(Label = ifelse(Item %in% c(1,2,7,12,14,15), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```

\pagebreak

Go back to [Methods](#ses)

```{r tableS6, tab.id="tableS6", tab.cap.style="Table Caption", tab.cap="Items of the perceived resource scarcity scale"}
codebook %>%
  filter(str_detect(Variable, "ses\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  mutate(Label = ifelse(!Item %in% c(7), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```

\pagebreak 

Go back to [Methods](#impulsivity)

```{r tableS7, tab.id="tableS7", tab.cap.style="Table Caption", tab.cap="Items of the Barrett Impulsivity Scale (BIS) - motor impulsivity subscale"}
codebook %>%
  filter(str_detect(Variable, "impuls\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description")
```

\pagebreak

Go back to [Methods](#future_orientation)

```{r tableS8, tab.id="tableS8", tab.cap.style="Table Caption", tab.cap="Items of the Future Orientation Scale (FOS)"}
codebook %>%
  filter(str_detect(Variable, "fos\\d\\d")) %>% 
  slice(match(c("fos01","fos06","fos07","fos12","fos13", "fos02","fos05","fos08","fos11","fos14", "fos03","fos04","fos09","fos10","fos15"), Variable)) %>%
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  add_row(.before = 1, "Label" = "Planning ahead") %>%
  add_row(.after = 6, "Label" = "Time perspective") %>%
  add_row(.after = 12, "Label" = "Anticipation of future consequences") %>%
  mutate(Label = ifelse(Item %in% c(1,2,5,8,9,11,12,15), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  merge_h_range(i = c(1,7,13)) %>%
  bold(i = c(1,7,13)) %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```

\pagebreak

Go back to [Methods](#depression)

```{r tableS9, tab.id="tableS9", tab.cap.style="Table Caption", tab.cap="Items of the Epidemiologic Studies Depression Scale (CESD)"}
codebook %>%
  filter(str_detect(Variable, "depression\\d\\d")) %>% 
  mutate(Item = 1:n()) %>%
  select(Item, Label) %>%
  add_column(.after = 1, "empty1" = "") %>%
  mutate(Label = ifelse(Item %in% c(4,8,12,16), str_c(Label, "*"), Label)) %>%
  flextable(cwidth = c(0.75, .2, 5)) %>%
  set_header_labels(Item = "Item", empty1 = "", Label = "Description") %>%
  add_footer_row(values = " ", colwidths = 3) %>%
  compose(
    i = 1, j = 1,
    as_paragraph(as_i("Note: "), "Reverse scored items."),
    part = 'footer')
```
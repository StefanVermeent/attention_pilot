---
title: "Supplements"
format: html
editor: visual
---

```{r include=FALSE}
load("../preregistrations/1_pilot/analysis_objects/supp_section2.Rdata")
load("../preregistrations/2_study1/analysis_objects/supp_section2.Rdata")
load("../preregistrations/3_study2/analysis_objects/supp_section2.Rdata")
load("../data/1_pilot/mod_fit_flanker.RData")
load("../data/3_study2/ssp_fit.RData")


source("../manuscript/pilot_stage_objects.R") # Staged results of the pilot study

library(flextable)

# set up flextable for tables
set_flextable_defaults(
  font.family = "Times", 
  font.size = 10,
  font.color = "black",
  line_spacing = 1,
  padding.bottom = 1, 
  padding.top = 1,
  padding.left = 1,
  padding.right = 1
)
```


# Section 1. Descriptions of exploratory measures

The following exploratory measures were collected in all three studies

## 1.1. Current state 
We assessed state anxiety during the experiment using the state subscale of the State-Trait Anxiety Inventory [STAI-S\; @spielberger_1999]. 
The STAI-S contains 20 short items measuring current anxiety (e.g., "I feel tense"). 
Participants rated each item on a scale of 1 (not at all) to 4 (very much so). 
An overall state anxiety variable was computed by averaging across the 20 unweighted items.

In addition, participants answered five questions relating to specific states: "Are you currently sick?" (rated as yes or no); "Have you eaten a full meal today?" (rated as yes or no); "How hungry do you feel right now?" (rated from 1 (not at all) to 5 (very hungry)); "How well did you sleep last night?" (rated from 1 (very poorly) to 5 (very well)); "How rested or refreshed did you feel when you woke up this morning?" (rated from 1 (not at all) to 5 (very rested)). We computed an overall sleep deprivation composite by standardizing and averaging across the two unweighted sleep-related items.

## 1.2. Poverty exposure 
Participants' perceived level of resource scarcity before age 13 was measured using seven items (e.g., "Your family had enough money to afford the kind of home you all needed"). 
Participants rated each item on a scale from 1 (never true) to 5 (very often true). 
Scores for the first six items were reverse coded so that higher scores indicated more perceived resource scarcity. 
The items were averaged together to create an unweighted composite scale.

In addition, we measured several indicators of objective SES before age 13. 
First, participants separately indicated the highest education of their mother and father on an 8-point scale: 'some high school', 'GED', 'high school diploma', 'some college but no college degree', associate's  degree', 'bachelor's degree', 'master's  degree', or 'doctoral or lab degree'. 
The mother and father education level were averaged to create an overall unweighted parental education composite. 
Participants also indicated their family's household income before age 13 on a 6-point scale: 'less than \$ 25k/year', '\$25k - \$49k/year, '\$50 - \$74k/year', '\$75 - \$99k/year', '\$100 - \$149k/year', 'more than \$150k/year'. 
Scores were reverse coded so that higher scores indicated higher levels of poverty.

We created a composite score of poverty exposure before age 13 by averaging together the standardized scores of perceived level of resource scarcity, overall parental education, and household income.

## 1.3. Impulsivity 
We assessed impulsivity with the Motor Impulsivity subscale of the Barrett Impulsivity Scale [BIS short form\; @patton_1995; @spinella_2007]. 
The Motor Impulsivity subscale of the BIS consists of five items (e.g., "I do things without thinking").
We did not include the Non-planning subscale because it overlapped substantially with the Future Orientation Scale described below. 
In addition, we did not include the Attention impulsivity subscale because it included items which we deemed to be mostly irrelevant for our target population (e.g., "I 'squirm' at plays or lectures"). 
We changed the original 4-point rating scale (rarely/never to almost always) to a 5-point rating scale ranging from 1 (never true) to 5 (very often true). 
An overall impulsivity variable was computed by averaging the five unweighted items.

## 1.4. Future Orientation 
We assessed future orientation with an adapted version of the Future Orientation Scale [FOS\; @steinberg_2009]. 
The original scale consists of 15 sets of opposing items separated by "BUT" (e.g., "Some people like to plan things out one step at a time BUT other people like to jump right into things without planning them out beforehand"). 
Participants first choose the item that best matches their general preference, and then indicate whether the statement is "really true" or "somewhat true". 
We adapted this format in a couple of ways. 
First, we converted the two statements per item to a single statement by picking the statements in the original right-hand column. 
Second, we adapted the 15 statements from a third-person to a first-person format. 
These changes were made in an attempt to reduce the cognitive load of the items. 
We worried that people with less formal education or who were sitting in a noisier environment would struggle with the length of the original items. 

In addition, item 8 of the original scale ("[...] other people would rather spend their money right away on something fun than save it for a rainy day") was changed to "I'd rather spend money right away than save it for a rainy day" (i.e., dropping the phrase "on something fun") to make it more general with regard to the thing that money is spent on. 
For people from adversity, spending money right now instead of saving it for the future might often be born out of necessity (e.g., having just enough money for food and shelter; being in debt) instead of a failure to delay gratification. 
Finally, the rating scale was adapted from the original 4-point scale (ranging from really true for the left-hand statement to really true for the right-hand statement) to a 5-point scale ranging from 1 (never true) to 5 (very often true).
An overall future orientation variable was computed by averaging the 15 unweighted items.

## 1.5. Depressive symptoms 
We assessed depressive symptoms during the past week using the Center for Epidemiologic Studies Depression Scale [CESD\; @radloff_1977]. 
The scale consists of 20 items (e.g., "I do things without thinking"). 
Participants rate each item on a scale of 1 (rarely or none of the time (less than 1 day)) to 4 (most or all of the time (5-7 days)). 
An overall depression variable was computed by averaging the 20 unweighted items.

# Section 2. Exploratory analyses

## 2.1. Consistency in unpredictability measures

### 2.1.1. Pilot

The EFA yielded five factors based on parallel analysis (see Table SX).
Based on their contents, we labelled these factors (1) Daily unpredictability; (2) Household routine; (3) Spatial unpredictability; (4) Chaos/clutter; (5) Social unpredictability.

```{r}
pilot_efa_table
```

### 2.1.2 Study 1

Similar to the Pilot, the EFA yielded five factors based on parallel analysis.
We plotted the factor loadings of each factor in the Pilot against those in Study 1 to investigate their correspondence (See Figure SX).
In general, individual items largely loaded on the same factors, and the sizes of their loadings were also comparable.
The items from the CHAOS were found to be most unstable, with many showing a loading < .32 in one of the two studies.

```{r}
study1_efa_fig
```
## 2.2. Bivariate correlations between future orientation and impulsivity with attention tasks

Table SX shows bivariate correlations between self-reported depression, impulsivity, future orientation, and SES with each of the Flanker SSP parameters and the Global-Local drift rate difference.
Participants who reported more depressive symptoms had a lower strength of perceptual input.
Participants who reported more impulsivity had a lower strength of perceptual input, higher interference, as well as a more holistic processing style.
Participants who were more future oriented had a higher strength of perceptual input and a more detail-oriented processing style, without an association with interference.

```{r}
supp_cor_table
```


# Section 3. Deviations from preregistrations

## 3.1. Pilot 

## 3.2. Study 1 

## 3.3. Study 2 


# Section 4. Model fit

## 4.1. Pilot 

## 4.1.1 Cueing and Change Detection Task

In our initial, preregistered approach, DDM models for the Cueing and Change Detection Task were fit with the fast-dm-30 software [@voss_2015] using maximum likelihood (ML) estimation.
For both tasks, we started out with a model that freely estimated all parameters, and then fit additional models with an increasing number of constrained parameters.
We compared model fit using the Bayesian information criterion (BIC), for which smaller values indicate better fit.
For the Change Detection Task, the most simple model provided the best fit.
This model freely estimated the drift rate, non-decision time, and boundary separation, and fixed all other parameters.

For the Cued Attention Task, three models provided comparable model fit.
However, all three models showed estimation problems, especially with regard to the boundary separation.
Specifically, boundary separation estimates for several participants ended up at an upper boundary of 10, indicating that they were not recovered well.
Based on subsequent external input, we fit an additional model using Kolmogorov-Smirnov (KS) estimation instead of ML estimation, additionally estimating the inter-trial variability parameter of the non-decision time.
This improved parameter estimation (see Figure SX).

```{r}
cueing_DDM_results_mod9 |> 
  select(starts_with("cueing"), -cueing_ks_fit) |> 
  rename(
    "a" = cueing_ks_a,
    "v - cued" = cueing_cued_ks_v,
    "v - neutral" = cueing_neutral_ks_v,
    "t - cued" = cueing_cued_ks_t0,
    "t - neutral" = cueing_neutral_ks_t0,
  ) |> 
  pivot_longer(everything(), names_to = "parameter", values_to = "Estimate") |> 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~parameter, scales = 'free')
```
Finally, we switched to estimation using Hierarchical Bayesian DDM (HDDM) for our final analyses.
The main reason for this step was that although KS estimation seemed to work well, we had fewer trials than is typically recommended for this estimation technique [@lerche_2017].
An advantage of HDDM is that it uses group-level estimates to inform and constrain individual-level estimates.
This is especially useful in cases such as ours, where we have a large sample size but relatively few trials per participant.

The HDDM models were fit using the *runjags* package [@denwood_2016], using code from @johnson_2017.
All models were fit using three Markov Chain Monte Carlo (MCMC) chains.
Each of these chains started with 2,000 burn-in samples, followed by 10,000 additional samples.
To decrease the total size of the model, every 10th sample was retained, resulting in a posterior sample of 3,000 samples.

Model convergence was assessed (1) by visually inspecting the traces, which should not contain any drifts or large jumps; (2) through simulation. Specifically, we used each participant's DDM estimates to simulate 100 RT and accuracy estimates (per condition). The distributions of the participant's true RTs and their simulated RTs were assessed through bivariate correlations at the 25th, 50th and 75th percentile.
We made the same comparison for mean accuracy levels.

## 4.1.2 Flanker

To fit the SSP model to the Flanker data, we followed recommendations by @grange_2016.
First, we searched for the optimal set of starting values.
For each participant, we used 50 sets of starting parameters with a variance of 20 for each, simulating 1,000 trials.
After finding the optimal starting values, we fit the final model based on 50,000 simulated trials.
Model fit was assessed through simulation.
For each participant, we simulated 50,000 trials.
We then calculated correlations between observed and simulated RTs at the 25th, 50th, and 75th percentile, as well as between observed and simulated mean accuracy.
As can be seen in Figure SX and Figure SX, we observed high agreement between observed and simulated RTs and accuracy rates.

<br>

```{r}
qq_data_flanker <- left_join(predicted_quantiles_flanker, observed_quantiles_flanker)

qq_flanker_rt <- ggplot(qq_data_flanker) +
  geom_point(aes(quan_rt_obs, quan_rt_pred)) +
  geom_abline(slope = 1, intercept = 0) +
  facet_grid(congruency~quantile) +
  labs(
    x = "\nObserved RT",
    y = "Predicted RT\n",
    title = "Flanker Task"
  ) +
  theme_classic()
```

<br>

```{r}
qq_flanker_acc <- ggplot(qq_data_flanker) +
  geom_point(aes(prop_acc_obs, prop_acc_pred)) +
  geom_abline(slope = 1, intercept = 0) +
  facet_grid(congruency~quantile) +
  expand_limits(x = 0, y = 0) +
  labs(
    x = "\nObserved Acc",
    y = "Predicted Acc\n"
  ) +
  theme_classic()
```

## 4.2. Study 1 

Model fit of the Flanker was done the same as in the Pilot.
Figure SX and Figure SX show the model fit based on simulated data.
We found good model fit across all three conditions, both for RTs as well as accuracy rates.

<br>

```{r}
study1_ssp_fit_rt
```

<br>

```{r}
study1_ssp_fit_acc
```

<br>

## 4.3. Study 2 

### 4.3.1 Flanker 

Model fit of the Flanker was done the same as in the Pilot and Study 1.
Figure SX and Figure SX show the model fit based on simulated data.
We found good model fit, both for RTs as well as accuracy rates.

```{r}
study2_ssp_fit_rt
```

<br>

```{r}
study2_ssp_fit_acc
```

### 4.3.2 Global-Local Task



# Section 5. Multiverse analysis.


